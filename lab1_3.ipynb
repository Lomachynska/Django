{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "lab1-3.ipynb",
      "authorship_tag": "ABX9TyNkLdm3EmJ+qEs9+mNQVqKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lomachynska/Django/blob/main/lab1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9tXvKE7OD16"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Завантаження та підготовка даних з датасету MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Перетворення даних для схожості з EMNIST\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# Нормалізація значень пікселів\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Перетворення міток класів у формат one-hot\n",
        "num_classes = len(np.unique(y_train))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Створення моделі багатошарового персептрону\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28, 1)),  # Вирівнювання вхідних даних\n",
        "    Dense(128, activation='relu'),   # Полно-зв'язаний шар з 128 нейронами та функцією активації ReLU\n",
        "    Dense(num_classes, activation='softmax')  # Вихідний шар з кількістю нейронів, що відповідають кількості класів та функцією активації softmax\n",
        "])\n",
        "\n",
        "# Компіляція моделі\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Навчання моделі\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8POPd_NbOLIg",
        "outputId": "a49adc54-9f6f-4a0f-d7f0-0b5a2d5421b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2612 - accuracy: 0.9266 - val_loss: 0.1408 - val_accuracy: 0.9572\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1149 - accuracy: 0.9666 - val_loss: 0.1017 - val_accuracy: 0.9693\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0781 - accuracy: 0.9767 - val_loss: 0.0867 - val_accuracy: 0.9739\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0585 - accuracy: 0.9818 - val_loss: 0.0821 - val_accuracy: 0.9742\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0461 - accuracy: 0.9857 - val_loss: 0.0758 - val_accuracy: 0.9763\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0365 - accuracy: 0.9887 - val_loss: 0.0734 - val_accuracy: 0.9786\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0301 - accuracy: 0.9906 - val_loss: 0.0732 - val_accuracy: 0.9784\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0233 - accuracy: 0.9928 - val_loss: 0.0730 - val_accuracy: 0.9789\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.0849 - val_accuracy: 0.9772\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0807 - val_accuracy: 0.9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, grid_size):\n",
        "        self.grid_size = grid_size\n",
        "        self.state = (0, 0)  # Початковий стан\n",
        "        self.goal = (grid_size-1, grid_size-1)  # Кінцевий стан\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)  # Початковий стан\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Рух вгору\n",
        "            self.state = (max(0, self.state[0]-1), self.state[1])\n",
        "        elif action == 1:  # Рух вниз\n",
        "            self.state = (min(self.grid_size-1, self.state[0]+1), self.state[1])\n",
        "        elif action == 2:  # Рух вліво\n",
        "            self.state = (self.state[0], max(0, self.state[1]-1))\n",
        "        elif action == 3:  # Рух вправо\n",
        "            self.state = (self.state[0], min(self.grid_size-1, self.state[1]+1))\n",
        "\n",
        "        reward = -1  # Нагорода за кожен крок\n",
        "\n",
        "        if self.state == self.goal:\n",
        "            reward = 0  # Нагорода за досягнення цілі\n",
        "\n",
        "        return self.state, reward\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_actions, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        self.num_actions = num_actions\n",
        "        self.epsilon = epsilon  # Ймовірність випадкової дії\n",
        "        self.alpha = alpha  # Крок навчання\n",
        "        self.gamma = gamma  # Дисконтний фактор\n",
        "\n",
        "        self.q_table = np.zeros((grid_size, grid_size, num_actions))  # Q-таблиця\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.num_actions)  # Випадкова дія\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Вибір дії з максимальним Q-значенням\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "        self.q_table[state][action] += self.alpha * (reward + self.gamma * next_max - self.q_table[state][action])\n",
        "\n",
        "# Параметри\n",
        "grid_size = 10\n",
        "num_actions = 4\n",
        "num_episodes = 1000\n",
        "\n",
        "# Ініціалізація середовища та агента\n",
        "env = Environment(grid_size)\n",
        "agent = QLearningAgent(num_actions)\n",
        "\n",
        "# Навчання\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward = env.step(action)\n",
        "        agent.update_q_table(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "\n",
        "        if state == env.goal:\n",
        "            done = True\n",
        "\n",
        "# Виведення оптимальної стратегії\n",
        "optimal_policy = np.argmax(agent.q_table, axis=2)\n",
        "print(\"Optimal Policy:\")\n",
        "print(optimal_policy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYkcjA8RY2Ny",
        "outputId": "cbb15e78-4062-45fd-ee1f-bc82e1b4d3c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[[1 1 3 3 1 0 1 1 1 1]\n",
            " [1 1 1 3 3 1 1 0 1 1]\n",
            " [1 0 1 3 3 3 3 2 3 1]\n",
            " [3 1 1 1 1 1 3 1 1 1]\n",
            " [1 1 3 3 1 3 1 3 3 1]\n",
            " [1 1 3 3 1 3 1 1 1 1]\n",
            " [1 1 3 1 3 1 1 3 3 1]\n",
            " [1 1 1 3 3 1 1 1 1 1]\n",
            " [1 1 1 3 3 3 1 3 1 1]\n",
            " [2 3 3 3 3 3 3 3 3 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GameState:\n",
        "    def __init__(self):\n",
        "        # Ініціалізуємо початковий стан гри\n",
        "        pass\n",
        "\n",
        "    def actions(self):\n",
        "        # Повертає список доступних дій у поточному стані гри\n",
        "        # Припустимо, що цей метод повертає список доступних ходів\n",
        "        return []\n",
        "\n",
        "    def result(self, action):\n",
        "        # Повертає новий стан гри, який виникає після виконання певної дії\n",
        "        # Припустимо, що цей метод змінює стан гри згідно з вибраним дією\n",
        "        pass\n",
        "\n",
        "    def utility(self):\n",
        "        # Обчислює корисність (виграш або програш) у поточному стані гри\n",
        "        # Припустимо, що цей метод повертає числове значення, що відображає виграш або програш\n",
        "        return 0\n",
        "\n",
        "    def terminal_test(self):\n",
        "        # Перевіряє, чи є поточний стан гри термінальним (кінцевим)\n",
        "        # Припустимо, що цей метод повертає True або False\n",
        "        return False\n",
        "\n",
        "\n",
        "def minimax_decision(state):\n",
        "    def max_value(state):\n",
        "        if state.terminal_test():\n",
        "            return state.utility()\n",
        "        v = float('-inf')\n",
        "        for action in state.actions():\n",
        "            v = max(v, min_value(state.result(action)))\n",
        "        return v\n",
        "\n",
        "    def min_value(state):\n",
        "        if state.terminal_test():\n",
        "            return state.utility()\n",
        "        v = float('inf')\n",
        "        for action in state.actions():\n",
        "            v = min(v, max_value(state.result(action)))\n",
        "        return v\n",
        "\n",
        "    best_action = None\n",
        "    best_value = float('-inf')\n",
        "\n",
        "    for action in state.actions():\n",
        "        value = min_value(state.result(action))\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "\n",
        "# Основний код\n",
        "if __name__ == \"__main__\":\n",
        "    initial_state = GameState()  # Ініціалізуємо початковий стан гри\n",
        "    best_action = minimax_decision(initial_state)  # Знаходимо оптимальну дію для гравця\n",
        "    print(\"Optimal action:\", best_action)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux2WGpcQp4zH",
        "outputId": "41cf0d9d-e860-4b7e-8aea-dca5cf681ce7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal action: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class ExpectimaxAgent:\n",
        "    def getAction(self, gameState):\n",
        "        \"\"\"\n",
        "        Вибір дії для даного стану гри.\n",
        "        :param gameState: Поточний стан гри\n",
        "        :return: Обрана дія\n",
        "        \"\"\"\n",
        "        # Отримуємо доступні дії для Пакмена\n",
        "        legalActions = self.getLegalActions(gameState)\n",
        "\n",
        "        # Вибираємо одну з доступних дій випадковим чином\n",
        "        action = random.choice(legalActions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def getLegalActions(self, gameState):\n",
        "        \"\"\"\n",
        "        Повертає список доступних дій для даного стану гри.\n",
        "        :param gameState: Поточний стан гри\n",
        "        :return: Список доступних дій\n",
        "        \"\"\"\n",
        "        # Реалізуйте логіку отримання доступних дій для Пакмена з поточного стану гри\n",
        "        pass\n",
        "\n",
        "    def getSuccessor(self, gameState, action):\n",
        "        \"\"\"\n",
        "        Повертає наступний стан гри після виконання певної дії.\n",
        "        :param gameState: Поточний стан гри\n",
        "        :param action: Дія, яку виконує агент\n",
        "        :return: Наступний стан гри\n",
        "        \"\"\"\n",
        "        # Реалізуйте логіку отримання наступного стану гри після виконання певної дії\n",
        "        pass\n",
        "\n",
        "    def isTerminal(self, gameState):\n",
        "        \"\"\"\n",
        "        Перевіряє, чи є поточний стан гри термінальним.\n",
        "        :param gameState: Поточний стан гри\n",
        "        :return: True, якщо гра закінчилася, інакше False\n",
        "        \"\"\"\n",
        "        # Реалізуйте логіку перевірки, чи є поточний стан гри термінальним\n",
        "        pass\n",
        "\n",
        "    def getUtility(self, gameState):\n",
        "        \"\"\"\n",
        "        Обчислює корисність (виграш або програш) для даного стану гри.\n",
        "        :param gameState: Поточний стан гри\n",
        "        :return: Числове значення, яке відображає корисність даного стану гри\n",
        "        \"\"\"\n",
        "        # Реалізуйте логіку обчислення корисності (виграш або програш) для даного стану гри\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "VIURIfK_rp8g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def betterEvaluationFunction(currentGameState):\n",
        "    \"\"\"\n",
        "    Оцінює поточний стан гри та повертає числове значення, яке відображає його привабливість для Пакмена.\n",
        "    \"\"\"\n",
        "    # Отримання інформації про стан гри\n",
        "    pacmanPosition = currentGameState.getPacmanPosition()\n",
        "    foodGrid = currentGameState.getFood()\n",
        "    capsules = currentGameState.getCapsules()\n",
        "    ghostStates = currentGameState.getGhostStates()\n",
        "\n",
        "    # Оцінка відстані до найближчої кульки\n",
        "    closestFood = min([manhattanDistance(pacmanPosition, food) for food in foodGrid.asList()])\n",
        "\n",
        "    # Оцінка кількості кульок та бігмаксів\n",
        "    numFood = currentGameState.getNumFood()\n",
        "    numCapsules = len(capsules)\n",
        "\n",
        "    # Оцінка відстані до привидів\n",
        "    ghostDistances = [manhattanDistance(pacmanPosition, ghost.getPosition()) for ghost in ghostStates]\n",
        "\n",
        "    # Оцінка = відстань до найближчої кульки - кількість кульок - кількість бігмаксів + мінімальна відстань до привидів\n",
        "    evaluation = closestFood - numFood - numCapsules + min(ghostDistances)\n",
        "\n",
        "    return evaluation\n"
      ],
      "metadata": {
        "id": "AFfGfD-as90l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniMaxAgent:\n",
        "    def getAction(self, gameState):\n",
        "        \"\"\"\n",
        "        Вибір дії для даного стану гри.\n",
        "        :param gameState: Поточний стан гри\n",
        "        :return: Обрана дія\n",
        "        \"\"\"\n",
        "        # Реалізуйте алгоритм MiniMax тут\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "ktMIRQZGtt5e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-FdF0T2V87Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num_ghosts in range(1, 6):\n",
        "    print(f\"Number of Ghosts: {num_ghosts}\")\n",
        "    # Запуск гри з заданою кількістю привидів та агентом MiniMax\n",
        "    # Отримання результатів та їх аналіз\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHXse2Ynt4by",
        "outputId": "b96bc95c-7eea-4db7-82ee-7ee69800d8b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Ghosts: 1\n",
            "Number of Ghosts: 2\n",
            "Number of Ghosts: 3\n",
            "Number of Ghosts: 4\n",
            "Number of Ghosts: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GameState:\n",
        "    def __init__(self, pacman_pos, ghost_pos, food_positions):\n",
        "        self.pacman_pos = pacman_pos\n",
        "        self.ghost_pos = ghost_pos\n",
        "        self.food_positions = food_positions\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return len(self.food_positions) == 0 or self.pacman_pos == self.ghost_pos\n",
        "\n",
        "    def get_legal_moves(self, agent):\n",
        "        if agent == 'pacman':\n",
        "            return ['up', 'down', 'left', 'right']\n",
        "        elif agent == 'ghost':\n",
        "            # Assuming ghost can move in all directions\n",
        "            return ['up', 'down', 'left', 'right']\n",
        "\n",
        "    def generate_successor(self, agent, action):\n",
        "        if agent == 'pacman':\n",
        "            new_pos = self.pacman_pos.copy()\n",
        "            if action == 'up':\n",
        "                new_pos[0] -= 1\n",
        "            elif action == 'down':\n",
        "                new_pos[0] += 1\n",
        "            elif action == 'left':\n",
        "                new_pos[1] -= 1\n",
        "            elif action == 'right':\n",
        "                new_pos[1] += 1\n",
        "            return GameState(new_pos, self.ghost_pos, self.food_positions)\n",
        "        elif agent == 'ghost':\n",
        "            # Similar logic for ghost's movement\n",
        "            pass\n",
        "\n",
        "    def evaluate(self):\n",
        "        # Simple evaluation function: number of remaining food pellets\n",
        "        return len(self.food_positions)\n",
        "\n",
        "def minimax(state, depth, maximizing_player):\n",
        "    if depth == 0 or state is None or state.is_terminal():\n",
        "        return state.evaluate() if state is not None else 0\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = float('-inf')\n",
        "        for move in state.get_legal_moves('pacman'):\n",
        "            successor = state.generate_successor('pacman', move)\n",
        "            eval = minimax(successor, depth - 1, False)\n",
        "            max_eval = max(max_eval, eval)\n",
        "        return max_eval\n",
        "    else:\n",
        "        min_eval = float('inf')\n",
        "        for move in state.get_legal_moves('ghost'):\n",
        "            successor = state.generate_successor('ghost', move)\n",
        "            eval = minimax(successor, depth - 1, True)\n",
        "            min_eval = min(min_eval, eval)\n",
        "        return min_eval\n",
        "\n",
        "# Example usage\n",
        "initial_state = GameState(pacman_pos=[0, 0], ghost_pos=[3, 3], food_positions=[[1, 1], [2, 2], [3, 1]])\n",
        "best_score = minimax(initial_state, depth=3, maximizing_player=True)\n",
        "print(\"Best score:\", best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO8xQCTj88r7",
        "outputId": "2a0a2555-cd17-4aac-bade-ec8782363032"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from game import MultiAgentSearchAgent, GameState\n",
        "\n",
        "class AlphaBetaAgent(MultiAgentSearchAgent):\n",
        "    def getAction(self, gameState: GameState) -> str:\n",
        "        alpha = float(\"-inf\")\n",
        "        beta = float(\"inf\")\n",
        "        bestScore = float(\"-inf\")\n",
        "        bestAction = None\n",
        "\n",
        "        for action in gameState.getLegalActions(0):  # 0 - індекс Pacman\n",
        "            successorState = gameState.generateSuccessor(0, action)\n",
        "            score = self.minValue(successorState, 0, 1, alpha, beta)\n",
        "            if score > bestScore:\n",
        "                bestScore = score\n",
        "                bestAction = action\n",
        "            alpha = max(alpha, bestScore)\n",
        "\n",
        "        return bestAction\n",
        "\n",
        "    def maxValue(self, state: GameState, depth: int, alpha: float, beta: float) -> float:\n",
        "        if state.isWin() or state.isLose() or depth == self.depth:\n",
        "            return self.evaluationFunction(state)\n",
        "\n",
        "        value = float(\"-inf\")\n",
        "        for action in state.getLegalActions(0):  # 0 - індекс Pacman\n",
        "            successorState = state.generateSuccessor(0, action)\n",
        "            value = max(value, self.minValue(successorState, depth, 1, alpha, beta))\n",
        "            if value >= beta:\n",
        "                return value\n",
        "            alpha = max(alpha, value)\n",
        "        return value\n",
        "\n",
        "    def minValue(self, state: GameState, depth: int, agentIndex: int, alpha: float, beta: float) -> float:\n",
        "        if state.isWin() or state.isLose() or depth == self.depth:\n",
        "            return self.evaluationFunction(state)\n",
        "\n",
        "        value = float(\"inf\")\n",
        "        for action in state.getLegalActions(agentIndex):\n",
        "            successorState = state.generateSuccessor(agentIndex, action)\n",
        "            if agentIndex == state.getNumAgents() - 1:\n",
        "                value = min(value, self.maxValue(successorState, depth + 1, alpha, beta))\n",
        "            else:\n",
        "                value = min(value, self.minValue(successorState, depth, agentIndex + 1, alpha, beta))\n",
        "            if value <= alpha:\n",
        "                return value\n",
        "            beta = min(beta, value)\n",
        "        return value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "n16eJBTmSiS9",
        "outputId": "b84bd7b3-5033-48c6-b2e2-8064a7a89b9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'game'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1ebb470f2cf3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiAgentSearchAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGameState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAlphaBetaAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiAgentSearchAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgameState\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGameState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'game'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}